# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details


import functools
import hashlib
import io
import json
import uuid
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Coroutine, Generator, Iterator, Literal
from unittest.mock import patch

import numpy as np
import pytest
import ruamel.yaml
from ecoscope_workflows_runner.app import app
from ecoscope_workflows_runner.testing import Case, CaseRunner
from ecoscope_workflows_runner.tracing import (
    build_context_headers,
    configure_tracer,
    make_otel_console_exporter_file_dst_kws,
)
from opentelemetry import trace
from PIL import Image
from playwright.async_api import async_playwright
from rattler import MatchSpec
from skimage.metrics import structural_similarity as ssim
from syrupy import SnapshotAssertion
from syrupy.extensions.image import PNGImageSnapshotExtension
from syrupy.extensions.json import JSONSnapshotExtension
from syrupy.location import PyTestLocation
from syrupy.terminal import reset
from syrupy.types import SerializedData, SnapshotIndex

ARTIFACTS = Path(__file__).parent.parent
SNAPSHOT_DIRNAME = ARTIFACTS.parent / "__results_snapshots__"
SNAPSHOT_DIFF_OUTPUT_DIRNAME = ARTIFACTS.parent / "__diff_output__"
TEST_CASES_YAML = ARTIFACTS.parent / "test-cases.yaml"
MATCHSPEC_OVERRIDE = "ecoscope-workflows-river-health-workflow"
IO_TASKS_IMPORTABLE_REFERENCES = [
    "ecoscope_workflows_ext_ecoscope.tasks.io.get_subjectgroup_observations",
]

yaml = ruamel.yaml.YAML(typ="safe")


def _parse_case(value: str) -> Case:
    all_cases = yaml.load(Path(TEST_CASES_YAML).read_text())
    assert value in all_cases, f"'case_name = {value}' not found in {TEST_CASES_YAML =}"
    return Case(**all_cases[value])


def pytest_addoption(parser: pytest.Parser):
    parser.addoption(
        "--case",
        action="append",
        default=[],
        help=(
            "Specify the test case to run. Must be field names in test-cases.yaml. "
            "Can be specified multiple times."
        ),
    )


def pytest_generate_tests(metafunc: pytest.Metafunc):
    if (
        "success_case" in metafunc.fixturenames
        or "failure_case" in metafunc.fixturenames
    ) and not metafunc.config.getoption("case"):
        raise ValueError("At least one --case must be specified.")

    all_cases = {c: _parse_case(c) for c in metafunc.config.getoption("case")}
    if "success_case" in metafunc.fixturenames:
        success_cases = {k: v for k, v in all_cases.items() if not v.raises}
        metafunc.parametrize(
            "success_case",
            list(success_cases.values()),
            ids=list(success_cases),
            scope="session",
        )
    elif "failure_case" in metafunc.fixturenames:
        failure_cases = {k: v for k, v in all_cases.items() if v.raises}
        metafunc.parametrize(
            "failure_case",
            list(failure_cases.values()),
            ids=list(failure_cases),
            scope="session",
        )


class CustomSnapshotDirnameMixin:
    @classmethod
    def dirname(cls, *, test_location: "PyTestLocation") -> str:
        case_id = next(
            c
            for c in test_location.item.config.getoption("case")
            if c in test_location.item.nodeid
        )
        return SNAPSHOT_DIRNAME.joinpath(case_id).absolute().as_posix()


class CustomJSONSnapshot(CustomSnapshotDirnameMixin, JSONSnapshotExtension):
    @classmethod
    def get_snapshot_name(
        cls, *, test_location: "PyTestLocation", index: "SnapshotIndex"
    ) -> str:
        original_name = JSONSnapshotExtension.get_snapshot_name(
            test_location=test_location, index=index
        )
        test_name = original_name.split("[").pop(0)
        execution_mode = next(
            s for s in original_name.split("-") if s in ["async", "sequential"]
        )
        hasdata = "nodata" if "nodata" in original_name else "data"
        specifier = hasdata + (f"-{execution_mode}" if "failure" in test_name else "")
        return test_name + f"[{specifier}]"


def _png_bytes_to_array(png_bytes: bytes) -> np.ndarray:
    img = Image.open(io.BytesIO(png_bytes))
    return np.array(img)


def _join_png_images(png_bytes1: bytes, png_bytes2: bytes) -> bytes:
    """Join two PNG images horizontally.

    Args:
    png_bytes1 (bytes): Byte string of the first PNG image
    png_bytes2 (bytes): Byte string of the second PNG image

    Returns:
    bytes: A byte string representing the combined PNG image
    """
    # Open images from byte strings
    img1 = Image.open(io.BytesIO(png_bytes1))
    img2 = Image.open(io.BytesIO(png_bytes2))

    # Determine the height of the combined image (using the taller image's height)
    # Calculate total width
    max_height = max(img1.height, img2.height)
    total_width = img1.width + img2.width

    # Create a new image with the combined dimensions
    # Paste the first image on the left and the second on the right
    combined_img = Image.new("RGBA", (total_width, max_height), (0, 0, 0, 0))
    combined_img.paste(img1, (0, (max_height - img1.height) // 2))
    combined_img.paste(img2, (img1.width, (max_height - img2.height) // 2))

    # Save the combined image to a byte stream
    output_stream = io.BytesIO()
    combined_img.save(output_stream, format="PNG")
    output_stream.seek(0)

    return output_stream.getvalue()


class CustomPNGSnapshot(CustomSnapshotDirnameMixin, PNGImageSnapshotExtension):
    @classmethod
    def get_snapshot_name(
        cls, *, test_location: "PyTestLocation", index: "SnapshotIndex"
    ) -> str:
        original_name = PNGImageSnapshotExtension.get_snapshot_name(
            test_location=test_location, index=index
        )
        test_name = original_name.split("[").pop(0)
        widget_name = original_name.split("[").pop(-1).split("]").pop(0)
        return f"{test_name}[{widget_name}]"

    @functools.cache
    def get_structural_similarity(
        self, serialized_data: bytes, snapshot_data: bytes
    ) -> float:
        serialized_arr = _png_bytes_to_array(serialized_data)
        snapshot_arr = _png_bytes_to_array(snapshot_data)
        return ssim(serialized_arr, snapshot_arr, multichannel=True, channel_axis=-1)

    def matches(self, *, serialized_data, snapshot_data):
        similarity = self.get_structural_similarity(serialized_data, snapshot_data)
        return similarity == 1.0

    def diff_lines(
        self, serialized_data: "SerializedData", snapshot_data: "SerializedData"
    ) -> Iterator[str]:
        # TODO: if AbstractSyrupyExtension.diff_lines signature is extended to be passed
        # test_location & index args from SnapshotAssertion, and SnapshotAssertion.get_assert_diff
        # passes those args to the extension.diff_lines call, then we can call self.get_location
        # with those args here and resolve a more human readable name.
        snapshot_data_hash = hashlib.sha256(snapshot_data).hexdigest()[0:7]
        serialized_data_hash = hashlib.sha256(serialized_data).hexdigest()[0:7]
        similarity = self.get_structural_similarity(serialized_data, snapshot_data)
        diff_image_fname = (
            f"{snapshot_data_hash}_{serialized_data_hash}_ssim{similarity}.diff.png"
        )
        diff_image_bytes = _join_png_images(snapshot_data, serialized_data)

        if not SNAPSHOT_DIFF_OUTPUT_DIRNAME.exists():
            SNAPSHOT_DIFF_OUTPUT_DIRNAME.mkdir(parents=True, exist_ok=True)
        for fname, data in [
            (diff_image_fname, diff_image_bytes),
            (f"{snapshot_data_hash}.png", snapshot_data),
            (f"{serialized_data_hash}.png", serialized_data),
        ]:
            SNAPSHOT_DIFF_OUTPUT_DIRNAME.joinpath(fname).write_bytes(data)

        for line in self._SnapshotReporter__diff_lines(str(1.0), str(similarity)):
            yield reset(line)


@pytest.fixture
def snapshot_json(snapshot: SnapshotAssertion) -> SnapshotAssertion:
    return snapshot.use_extension(CustomJSONSnapshot)


@pytest.fixture
def snapshot_png(snapshot: SnapshotAssertion) -> SnapshotAssertion:
    return snapshot.use_extension(CustomPNGSnapshot)


@pytest.fixture(scope="session")
def results_dir(tmp_path_factory: pytest.TempPathFactory) -> Path:
    results_dir = tmp_path_factory.mktemp("results")
    return results_dir


@dataclass
class RunParams:
    api: Literal["app", "cli"]
    execution_mode: Literal["async", "sequential"]
    mock_io: bool = True

    @property
    def subdir_name(self):
        mock_io = "mock-io" if self.mock_io else "no-mock-io"
        return f"{self.api}-{self.execution_mode}-{mock_io}"


@pytest.fixture(
    scope="session",
    params=[
        RunParams(api="app", execution_mode="async"),
        RunParams(api="app", execution_mode="sequential"),
        RunParams(api="cli", execution_mode="async"),
        RunParams(api="cli", execution_mode="sequential"),
    ],
    ids=[
        "app-async-mock-io",
        "app-sequential-mock-io",
        "cli-async-mock-io",
        "cli-sequential-mock-io",
    ],
)
def run_params(request: pytest.FixtureRequest) -> RunParams:
    return request.param


def _run_test_case(
    run_params: RunParams,
    case: Case,
    results_subdir: Path,
    matchspec_override: str,
    data_connections_env_vars: dict | None = None,
    traceparent: str | None = None,
) -> dict:
    results_subdir.mkdir(parents=True, exist_ok=True)
    case_runner = CaseRunner(
        execution_mode=run_params.execution_mode,
        mock_io=run_params.mock_io,
        case=case,
        results_subdir=results_subdir,
        traceparent=traceparent,
    )
    match run_params.api:
        case "app":
            with patch.dict(
                "os.environ",
                {
                    "ECOSCOPE_WORKFLOWS_MATCHSPEC_OVERRIDE": matchspec_override,
                    "ECOSCOPE_WORKFLOWS_OTEL_EXPORTER": "console",
                    "ECOSCOPE_WORKFLOWS_OTEL_CONSOLE_EXPORTER_DST": "file",
                    "ECOSCOPE_WORKFLOWS_OTEL_CONSOLE_EXPORTER_FILE_DST_TARGET_DIR": results_subdir.as_posix(),
                },
            ):
                return case_runner.run_app(
                    app, data_connections_env_vars=data_connections_env_vars
                )
        case "cli":
            if case.raises:
                pytest.skip("CLI tests do not yet support error handling.")
            return case_runner.run_cli(matchspec=MatchSpec(matchspec_override))
        case _ as unknown:
            raise ValueError(f"Unknown API: {unknown}")


@pytest.fixture(scope="session")
def matchspec_override() -> str:
    return MATCHSPEC_OVERRIDE


@pytest.fixture(scope="session", params=[True, False], ids=["nodata", "data"])
def no_data(request: pytest.FixtureRequest) -> bool:
    """Fixture to control whether data is null or not."""
    return request.param


@pytest.fixture(scope="session")
def io_tasks_importable_references() -> list[str]:
    """Fixture to provide importable references for all io tasks in this workflow."""
    return IO_TASKS_IMPORTABLE_REFERENCES


def _make_results_subdir(
    case: Case,
    *,
    run_params: RunParams,
    results_dir: Path,
    no_data: bool | None = None,
) -> Path:
    match no_data:
        case None:
            hasdata = ""
        case True:
            hasdata = "-nodata"
        case False:
            hasdata = "-data"
        case _:
            raise ValueError(f"Unknown no_data value: {no_data}")
    name = case.name.lower().replace(" ", "-") + hasdata
    return results_dir / run_params.subdir_name / name / uuid.uuid4().hex


@pytest.fixture(scope="session")
def results_subdir_kws(
    run_params: RunParams,
    results_dir: Path,
) -> dict[str, Any]:
    return {
        "run_params": run_params,
        "results_dir": results_dir,
    }


@pytest.fixture(scope="session")
def results_subdir_success(
    success_case: Case, results_subdir_kws: dict, no_data: bool
) -> Path:
    return _make_results_subdir(success_case, no_data=no_data, **results_subdir_kws)


@pytest.fixture(scope="session")
def results_subdir_failure(failure_case: Case, results_subdir_kws: dict) -> Path:
    return _make_results_subdir(failure_case, **results_subdir_kws)


@pytest.fixture(scope="session")
def conftest_tracer_dst(results_dir: Path):
    return results_dir / "otel_traces.jsonl"


@pytest.fixture(scope="session")
def conftest_tracer(conftest_tracer_dst: Path) -> trace.Tracer:
    otel_exporter_kws = make_otel_console_exporter_file_dst_kws(
        target_dir=conftest_tracer_dst.parent,
    )
    configure_tracer("conftest", exporter="console", exporter_kws=otel_exporter_kws)
    return trace.get_tracer(__name__)


@pytest.fixture(scope="session")
def response_json_success(
    run_params: RunParams,
    success_case: Case,
    results_subdir_success: Path,
    matchspec_override: str,
    no_data: bool,
    tmp_path_factory: pytest.TempPathFactory,
    io_tasks_importable_references: list[str],
    conftest_tracer: trace.Tracer,
    conftest_tracer_dst: Path,
) -> Generator[dict]:
    data_connections_env_vars = None
    if no_data:
        import pandas as pd

        mock_io_dir = tmp_path_factory.mktemp("mock-io")
        example_return_path = mock_io_dir.joinpath("empty.parquet")
        pd.DataFrame().to_parquet(example_return_path)
        data_connections_env_vars = {
            f"ECOSCOPE_WORKFLOWS_MOCK_IO__{ref.replace('.', '_').upper()}": example_return_path.as_posix()
            for ref in io_tasks_importable_references
        }
    with conftest_tracer.start_as_current_span(
        "response_json_success_pytest_fixture",
        attributes={
            "this simulates": "traceparent propagation from the FastAPI app",
        },
    ):
        headers = build_context_headers()
        traceparent = headers.get("traceparent")
        result = _run_test_case(
            run_params,
            success_case,
            results_subdir_success,
            matchspec_override,
            data_connections_env_vars,
            traceparent=traceparent,
        )
    # exit context to close span, and then flush tracer provider
    provider = trace.get_tracer_provider()
    provider.force_flush()
    yield result
    if conftest_tracer_dst.exists():
        with conftest_tracer_dst.open("r+") as f:
            f.truncate(0)


@pytest.fixture(scope="session")
def response_json_failure(
    run_params: RunParams,
    failure_case: Case,
    results_subdir_failure: Path,
    matchspec_override: str,
) -> dict:
    return _run_test_case(
        run_params, failure_case, results_subdir_failure, matchspec_override
    )


def _iframe_widgets_from_response_json(response_json: dict) -> list[dict]:
    first_view = list(response_json["result"]["views"])[0]
    return [
        widget
        for widget in response_json["result"]["views"][first_view]
        if isinstance(widget["data"], str) and widget["data"].endswith(".html")
    ]


async def _take_screenshot(widget: dict) -> tuple[str, bytes]:
    assert widget["widget_type"] in ["map", "graph", "table"]
    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        await page.set_viewport_size({"width": 640, "height": 360})
        await page.goto(Path(widget["data"]).as_uri())
        if widget["widget_type"] == "map":
            await page.wait_for_timeout(20_000)
        await page.wait_for_timeout(10_000)
        png_bytes = await page.screenshot(full_page=True, timeout=0)
        return widget["title"], png_bytes


@pytest.fixture(scope="session")
def screenshot_coros(
    response_json_success: dict,
) -> list[Coroutine[Any, Any, tuple[str, bytes]]]:
    iframe_widgets = _iframe_widgets_from_response_json(response_json_success)
    return [_take_screenshot(widget) for widget in iframe_widgets]


@dataclass
class ReconstructedOtelSpan:
    trace_id: str
    span_id: str
    name: str
    parent_id: str | None
    start_time: str
    end_time: str
    attributes: dict[str, Any]
    status: dict[str, Any]

    @classmethod
    def from_json_line(cls, line: str) -> "ReconstructedOtelSpan":
        json_: dict = json.loads(line)
        ctx: dict = json_["context"]
        return cls(
            trace_id=ctx["trace_id"],
            span_id=ctx["span_id"],
            name=json_["name"],
            parent_id=json_.get("parent_id"),
            start_time=json_["start_time"],
            end_time=json_["end_time"],
            attributes=json_.get("attributes", {}),
            status=json_.get("status", {}),
        )


@pytest.fixture(scope="session")
def otel_traces_success(
    conftest_tracer_dst: Path,
    results_subdir_success: Path,
    response_json_success: dict,
) -> list[ReconstructedOtelSpan]:
    """Fixture to load OTEL traces from the results directory."""
    assert isinstance(response_json_success, dict)
    traces = []
    cli_tracer_dst = results_subdir_success / "otel_traces.jsonl"
    for dst in [conftest_tracer_dst, cli_tracer_dst]:
        if dst.exists():
            with dst.open("r") as f:
                for line in f:
                    rspan = ReconstructedOtelSpan.from_json_line(line)
                    traces.append(rspan)
    return traces
