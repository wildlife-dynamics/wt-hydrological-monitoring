# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details


# ruff: noqa: E402

# %% [markdown]
# # Hydrological Monitoring
# TODO: top level description

# %% [markdown]
# ## Imports

import os

from ecoscope_workflows_core.tasks.config import (
    set_workflow_details as set_workflow_details,
)
from ecoscope_workflows_core.tasks.filter import (
    get_timezone_from_time_range as get_timezone_from_time_range,
)
from ecoscope_workflows_core.tasks.filter import set_time_range as set_time_range
from ecoscope_workflows_core.tasks.groupby import set_groupers as set_groupers
from ecoscope_workflows_core.tasks.groupby import split_groups as split_groups
from ecoscope_workflows_core.tasks.io import persist_text as persist_text
from ecoscope_workflows_core.tasks.io import set_er_connection as set_er_connection
from ecoscope_workflows_core.tasks.results import (
    create_plot_widget_single_view as create_plot_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import gather_dashboard as gather_dashboard
from ecoscope_workflows_core.tasks.results import (
    merge_widget_views as merge_widget_views,
)
from ecoscope_workflows_core.tasks.transformation import (
    add_temporal_index as add_temporal_index,
)
from ecoscope_workflows_core.tasks.transformation import (
    convert_values_to_timezone as convert_values_to_timezone,
)
from ecoscope_workflows_core.tasks.transformation import (
    extract_column_as_type as extract_column_as_type,
)
from ecoscope_workflows_core.tasks.transformation import map_columns as map_columns
from ecoscope_workflows_ext_custom.tasks.io import (
    persist_df_wrapper as persist_df_wrapper,
)
from ecoscope_workflows_ext_custom.tasks.transformation import (
    apply_sql_query as apply_sql_query,
)
from ecoscope_workflows_ext_custom.tasks.transformation import (
    drop_column_prefix as drop_column_prefix,
)
from ecoscope_workflows_ext_ecoscope.tasks.analysis import summarize_df as summarize_df
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    get_subjectgroup_observations as get_subjectgroup_observations,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import (
    draw_line_chart as draw_line_chart,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    normalize_json_column as normalize_json_column,
)

# %% [markdown]
# ## Set Workflow Details

# %%
# parameters

workflow_details_params = dict(
    name=...,
    description=...,
    image_url=...,
)

# %%
# call the task


workflow_details = (
    set_workflow_details.set_task_instance_id("workflow_details")
    .handle_errors()
    .with_tracing()
    .partial(**workflow_details_params)
    .call()
)


# %% [markdown]
# ## Time Range

# %%
# parameters

time_range_params = dict(
    since=...,
    until=...,
    timezone=...,
)

# %%
# call the task


time_range = (
    set_time_range.set_task_instance_id("time_range")
    .handle_errors()
    .with_tracing()
    .partial(time_format="%d %b %Y %H:%M:%S %Z", **time_range_params)
    .call()
)


# %% [markdown]
# ## Extract Timezone Selection

# %%
# parameters

get_timezone_params = dict()

# %%
# call the task


get_timezone = (
    get_timezone_from_time_range.set_task_instance_id("get_timezone")
    .handle_errors()
    .with_tracing()
    .partial(time_range=time_range, **get_timezone_params)
    .call()
)


# %% [markdown]
# ## Set Groupers for Analysis

# %%
# parameters

groupers_params = dict(
    groupers=...,
)

# %%
# call the task


groupers = (
    set_groupers.set_task_instance_id("groupers")
    .handle_errors()
    .with_tracing()
    .partial(**groupers_params)
    .call()
)


# %% [markdown]
# ## Select EarthRanger Data Source

# %%
# parameters

er_client_name_params = dict(
    data_source=...,
)

# %%
# call the task


er_client_name = (
    set_er_connection.set_task_instance_id("er_client_name")
    .handle_errors()
    .with_tracing()
    .partial(**er_client_name_params)
    .call()
)


# %% [markdown]
# ## Get Subject Group Observations from EarthRanger

# %%
# parameters

subject_obs_stevens_params = dict()

# %%
# call the task


subject_obs_stevens = (
    get_subjectgroup_observations.set_task_instance_id("subject_obs_stevens")
    .handle_errors()
    .with_tracing()
    .partial(
        client=er_client_name,
        time_range=time_range,
        raise_on_empty=True,
        include_details=True,
        include_subjectsource_details=True,
        subject_group_name="Stevens Connect",
        **subject_obs_stevens_params,
    )
    .call()
)


# %% [markdown]
# ## Remove Extra Column Prefix

# %%
# parameters

drop_extra_prefix_stevens_params = dict()

# %%
# call the task


drop_extra_prefix_stevens = (
    drop_column_prefix.set_task_instance_id("drop_extra_prefix_stevens")
    .handle_errors()
    .with_tracing()
    .partial(
        df=subject_obs_stevens,
        prefix="extra__",
        duplicate_strategy="suffix",
        **drop_extra_prefix_stevens_params,
    )
    .call()
)


# %% [markdown]
# ## Preprocess Columns

# %%
# parameters

process_columns_stevens_params = dict()

# %%
# call the task


process_columns_stevens = (
    map_columns.set_task_instance_id("process_columns_stevens")
    .handle_errors()
    .with_tracing()
    .partial(
        df=drop_extra_prefix_stevens,
        drop_columns=[],
        retain_columns=[
            "id",
            "subject_id",
            "created_at",
            "recorded_at",
            "source",
            "device_status_properties",
            "observation_details",
            "geometry",
            "subject__name",
        ],
        rename_columns={"subject__name": "sensor"},
        **process_columns_stevens_params,
    )
    .call()
)


# %% [markdown]
# ## Convert to timezone

# %%
# parameters

convert_to_user_timezone_stevens_params = dict()

# %%
# call the task


convert_to_user_timezone_stevens = (
    convert_values_to_timezone.set_task_instance_id("convert_to_user_timezone_stevens")
    .handle_errors()
    .with_tracing()
    .partial(
        df=process_columns_stevens,
        timezone=get_timezone,
        columns=["time"],
        **convert_to_user_timezone_stevens_params,
    )
    .call()
)


# %% [markdown]
# ## Normalize Observation Details

# %%
# parameters

normalize_obs_details_stevens_params = dict(
    sort_columns=...,
)

# %%
# call the task


normalize_obs_details_stevens = (
    normalize_json_column.set_task_instance_id("normalize_obs_details_stevens")
    .handle_errors()
    .with_tracing()
    .partial(
        df=convert_to_user_timezone_stevens,
        column="observation_details",
        skip_if_not_exists=False,
        **normalize_obs_details_stevens_params,
    )
    .call()
)


# %% [markdown]
# ## Remove Column Prefix Observation Details

# %%
# parameters

drop_obs_details_prefix_stevens_params = dict()

# %%
# call the task


drop_obs_details_prefix_stevens = (
    drop_column_prefix.set_task_instance_id("drop_obs_details_prefix_stevens")
    .handle_errors()
    .with_tracing()
    .partial(
        df=normalize_obs_details_stevens,
        prefix="observation_details__",
        duplicate_strategy="suffix",
        **drop_obs_details_prefix_stevens_params,
    )
    .call()
)


# %% [markdown]
# ## Extract Date

# %%
# parameters

extract_date_stevens_params = dict()

# %%
# call the task


extract_date_stevens = (
    extract_column_as_type.set_task_instance_id("extract_date_stevens")
    .handle_errors()
    .with_tracing()
    .partial(
        df=drop_obs_details_prefix_stevens,
        column_name="recorded_at",
        output_type="date",
        output_column_name="date",
        **extract_date_stevens_params,
    )
    .call()
)


# %% [markdown]
# ## Add temporal index

# %%
# parameters

df_with_temporal_index_params = dict()

# %%
# call the task


df_with_temporal_index = (
    add_temporal_index.set_task_instance_id("df_with_temporal_index")
    .handle_errors()
    .with_tracing()
    .partial(
        df=extract_date_stevens,
        time_col="recorded_at",
        groupers=groupers,
        cast_to_datetime=True,
        format="mixed",
        **df_with_temporal_index_params,
    )
    .call()
)


# %% [markdown]
# ## Split by Group

# %%
# parameters

split_river_groups_params = dict()

# %%
# call the task


split_river_groups = (
    split_groups.set_task_instance_id("split_river_groups")
    .handle_errors()
    .with_tracing()
    .partial(df=df_with_temporal_index, groupers=groupers, **split_river_groups_params)
    .call()
)


# %% [markdown]
# ## Persist Observations from Stevens Connect

# %%
# parameters

persist_stevens_observations_params = dict(
    filename=...,
    filetypes=...,
    filename_prefix=...,
    sanitize=...,
)

# %%
# call the task


persist_stevens_observations = (
    persist_df_wrapper.set_task_instance_id("persist_stevens_observations")
    .handle_errors()
    .with_tracing()
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        **persist_stevens_observations_params,
    )
    .mapvalues(argnames=["df"], argvalues=split_river_groups)
)


# %% [markdown]
# ## Extract River Values

# %%
# parameters

extract_river_values_params = dict()

# %%
# call the task


extract_river_values = (
    apply_sql_query.set_task_instance_id("extract_river_values")
    .handle_errors()
    .with_tracing()
    .partial(
        columns=["id", "DO", "Depth m", "date", "sensor"],
        query='SELECT CASE WHEN INSTR("DO", CHAR(32)) > 0 THEN CAST(SUBSTR("DO", 1, INSTR("DO", CHAR(32)) - 1) AS REAL) ELSE NULL END AS do_value, CASE WHEN INSTR("Depth m", CHAR(32)) > 0 THEN CAST(SUBSTR("Depth m", 1, INSTR("Depth m", CHAR(32)) - 1) AS REAL) ELSE NULL END AS depth_m_value, date, id FROM df WHERE sensor = "Mara River Purungat Bridge - Sensor M 20"',
        **extract_river_values_params,
    )
    .mapvalues(argnames=["df"], argvalues=split_river_groups)
)


# %% [markdown]
# ## Calculate Daily River Summary

# %%
# parameters

daily_river_params = dict()

# %%
# call the task


daily_river = (
    summarize_df.set_task_instance_id("daily_river")
    .handle_errors()
    .with_tracing()
    .partial(
        groupby_cols=["date"],
        summary_params=[
            {
                "display_name": "Depth (m)",
                "aggregator": "mean",
                "column": "depth_m_value",
            },
            {
                "display_name": "Dissolved Oxygen (mg/L)",
                "aggregator": "mean",
                "column": "do_value",
            },
        ],
        reset_index=True,
        **daily_river_params,
    )
    .mapvalues(argnames=["df"], argvalues=extract_river_values)
)


# %% [markdown]
# ## Persist Daily Summary from Stevens Connect

# %%
# parameters

persist_daily_summary_stevens_params = dict(
    filename=...,
    filename_prefix=...,
    sanitize=...,
)

# %%
# call the task


persist_daily_summary_stevens = (
    persist_df_wrapper.set_task_instance_id("persist_daily_summary_stevens")
    .handle_errors()
    .with_tracing()
    .partial(
        root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
        filetypes=["csv"],
        **persist_daily_summary_stevens_params,
    )
    .mapvalues(argnames=["df"], argvalues=daily_river)
)


# %% [markdown]
# ## Draw River Depth Chart

# %%
# parameters

depth_chart_params = dict(
    widget_id=...,
)

# %%
# call the task


depth_chart = (
    draw_line_chart.set_task_instance_id("depth_chart")
    .handle_errors()
    .with_tracing()
    .partial(
        x_column="date",
        y_column="Depth (m)",
        line_kwargs={"shape": "spline"},
        layout_kwargs={
            "xaxis": {"title": "Date"},
            "yaxis": {"title": "Depth (m)"},
            "legend_title": "Weather Station",
            "hovermode": "closest",
        },
        category_column="",
        **depth_chart_params,
    )
    .mapvalues(argnames=["dataframe"], argvalues=daily_river)
)


# %% [markdown]
# ## Persist Depth Chart as Text

# %%
# parameters

persist_depth_params = dict(
    filename=...,
    filename_suffix=...,
)

# %%
# call the task


persist_depth = (
    persist_text.set_task_instance_id("persist_depth")
    .handle_errors()
    .with_tracing()
    .partial(root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"], **persist_depth_params)
    .mapvalues(argnames=["text"], argvalues=depth_chart)
)


# %% [markdown]
# ## Create Depth Widget

# %%
# parameters

depth_chart_widget_params = dict()

# %%
# call the task


depth_chart_widget = (
    create_plot_widget_single_view.set_task_instance_id("depth_chart_widget")
    .handle_errors()
    .with_tracing()
    .partial(title="Daily River Flow", **depth_chart_widget_params)
    .map(argnames=["view", "data"], argvalues=persist_depth)
)


# %% [markdown]
# ## Merge Depth Widget Views

# %%
# parameters

grouped_depth_widget_params = dict()

# %%
# call the task


grouped_depth_widget = (
    merge_widget_views.set_task_instance_id("grouped_depth_widget")
    .handle_errors()
    .with_tracing()
    .partial(widgets=depth_chart_widget, **grouped_depth_widget_params)
    .call()
)


# %% [markdown]
# ## Draw DO Chart

# %%
# parameters

do_chart_params = dict(
    widget_id=...,
)

# %%
# call the task


do_chart = (
    draw_line_chart.set_task_instance_id("do_chart")
    .handle_errors()
    .with_tracing()
    .partial(
        x_column="date",
        y_column="Dissolved Oxygen (mg/L)",
        line_kwargs={"shape": "spline"},
        layout_kwargs={
            "xaxis": {"title": "Date"},
            "yaxis": {"title": "Average Daily Dissolved Oxygen (mg/L)"},
            "legend_title": "Weather Station",
            "hovermode": "closest",
        },
        category_column="",
        **do_chart_params,
    )
    .mapvalues(argnames=["dataframe"], argvalues=daily_river)
)


# %% [markdown]
# ## Persist DO Chart as Text

# %%
# parameters

persist_do_params = dict(
    filename=...,
    filename_suffix=...,
)

# %%
# call the task


persist_do = (
    persist_text.set_task_instance_id("persist_do")
    .handle_errors()
    .with_tracing()
    .partial(root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"], **persist_do_params)
    .mapvalues(argnames=["text"], argvalues=do_chart)
)


# %% [markdown]
# ## Create DO Widget

# %%
# parameters

do_chart_widget_params = dict()

# %%
# call the task


do_chart_widget = (
    create_plot_widget_single_view.set_task_instance_id("do_chart_widget")
    .handle_errors()
    .with_tracing()
    .partial(title="Daily Dissolved Oxygen", **do_chart_widget_params)
    .map(argnames=["view", "data"], argvalues=persist_do)
)


# %% [markdown]
# ## Merge DO Widget Views

# %%
# parameters

grouped_do_widget_params = dict()

# %%
# call the task


grouped_do_widget = (
    merge_widget_views.set_task_instance_id("grouped_do_widget")
    .handle_errors()
    .with_tracing()
    .partial(widgets=do_chart_widget, **grouped_do_widget_params)
    .call()
)


# %% [markdown]
# ## Create A Weather Dashboard

# %%
# parameters

weather_dashboard_params = dict(
    warning=...,
)

# %%
# call the task


weather_dashboard = (
    gather_dashboard.set_task_instance_id("weather_dashboard")
    .handle_errors()
    .with_tracing()
    .partial(
        details=workflow_details,
        widgets=[grouped_depth_widget, grouped_do_widget],
        time_range=time_range,
        groupers=groupers,
        **weather_dashboard_params,
    )
    .call()
)
