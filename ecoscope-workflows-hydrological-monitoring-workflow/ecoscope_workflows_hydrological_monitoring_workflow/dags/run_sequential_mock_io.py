# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details

# ruff: noqa: E402

"""WARNING: This file is generated in a testing context and should not be used in production.
Lines specific to the testing context are marked with a test tube emoji (ðŸ§ª) to indicate
that they would not be included (or would be different) in the production version of this file.
"""

import json
import os
import warnings  # ðŸ§ª

from ecoscope_workflows_core.tasks.config import (
    set_workflow_details as set_workflow_details,
)
from ecoscope_workflows_core.tasks.filter import (
    get_timezone_from_time_range as get_timezone_from_time_range,
)
from ecoscope_workflows_core.tasks.filter import set_time_range as set_time_range
from ecoscope_workflows_core.tasks.groupby import set_groupers as set_groupers
from ecoscope_workflows_core.tasks.io import set_er_connection as set_er_connection
from ecoscope_workflows_core.testing import create_task_magicmock  # ðŸ§ª

get_subjectgroup_observations = create_task_magicmock(  # ðŸ§ª
    anchor="ecoscope_workflows_ext_ecoscope.tasks.io",  # ðŸ§ª
    func_name="get_subjectgroup_observations",  # ðŸ§ª
)  # ðŸ§ª
from ecoscope_workflows_core.tasks.groupby import split_groups as split_groups
from ecoscope_workflows_core.tasks.io import persist_text as persist_text
from ecoscope_workflows_core.tasks.results import (
    create_plot_widget_single_view as create_plot_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import gather_dashboard as gather_dashboard
from ecoscope_workflows_core.tasks.results import (
    merge_widget_views as merge_widget_views,
)
from ecoscope_workflows_core.tasks.transformation import (
    add_temporal_index as add_temporal_index,
)
from ecoscope_workflows_core.tasks.transformation import (
    convert_values_to_timezone as convert_values_to_timezone,
)
from ecoscope_workflows_core.tasks.transformation import (
    extract_column_as_type as extract_column_as_type,
)
from ecoscope_workflows_core.tasks.transformation import map_columns as map_columns
from ecoscope_workflows_ext_custom.tasks.io import (
    persist_df_wrapper as persist_df_wrapper,
)
from ecoscope_workflows_ext_custom.tasks.transformation import (
    apply_sql_query as apply_sql_query,
)
from ecoscope_workflows_ext_custom.tasks.transformation import (
    drop_column_prefix as drop_column_prefix,
)
from ecoscope_workflows_ext_ecoscope.tasks.analysis import summarize_df as summarize_df
from ecoscope_workflows_ext_ecoscope.tasks.results import (
    draw_line_chart as draw_line_chart,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    normalize_json_column as normalize_json_column,
)

from ..params import Params


def main(params: Params):
    warnings.warn("This test script should not be used in production!")  # ðŸ§ª

    params_dict = json.loads(params.model_dump_json(exclude_unset=True))

    workflow_details = (
        set_workflow_details.validate()
        .set_task_instance_id("workflow_details")
        .handle_errors()
        .with_tracing()
        .partial(**(params_dict.get("workflow_details") or {}))
        .call()
    )

    time_range = (
        set_time_range.validate()
        .set_task_instance_id("time_range")
        .handle_errors()
        .with_tracing()
        .partial(
            time_format="%d %b %Y %H:%M:%S %Z", **(params_dict.get("time_range") or {})
        )
        .call()
    )

    get_timezone = (
        get_timezone_from_time_range.validate()
        .set_task_instance_id("get_timezone")
        .handle_errors()
        .with_tracing()
        .partial(time_range=time_range, **(params_dict.get("get_timezone") or {}))
        .call()
    )

    groupers = (
        set_groupers.validate()
        .set_task_instance_id("groupers")
        .handle_errors()
        .with_tracing()
        .partial(**(params_dict.get("groupers") or {}))
        .call()
    )

    er_client_name = (
        set_er_connection.validate()
        .set_task_instance_id("er_client_name")
        .handle_errors()
        .with_tracing()
        .partial(**(params_dict.get("er_client_name") or {}))
        .call()
    )

    subject_obs_stevens = (
        get_subjectgroup_observations.validate()
        .set_task_instance_id("subject_obs_stevens")
        .handle_errors()
        .with_tracing()
        .partial(
            client=er_client_name,
            time_range=time_range,
            raise_on_empty=True,
            include_details=True,
            include_subjectsource_details=True,
            subject_group_name="Stevens Connect",
            **(params_dict.get("subject_obs_stevens") or {}),
        )
        .call()
    )

    drop_extra_prefix_stevens = (
        drop_column_prefix.validate()
        .set_task_instance_id("drop_extra_prefix_stevens")
        .handle_errors()
        .with_tracing()
        .partial(
            df=subject_obs_stevens,
            prefix="extra__",
            duplicate_strategy="suffix",
            **(params_dict.get("drop_extra_prefix_stevens") or {}),
        )
        .call()
    )

    process_columns_stevens = (
        map_columns.validate()
        .set_task_instance_id("process_columns_stevens")
        .handle_errors()
        .with_tracing()
        .partial(
            df=drop_extra_prefix_stevens,
            drop_columns=[],
            retain_columns=[
                "id",
                "subject_id",
                "created_at",
                "recorded_at",
                "source",
                "device_status_properties",
                "observation_details",
                "geometry",
                "subject__name",
            ],
            rename_columns={"subject__name": "sensor"},
            **(params_dict.get("process_columns_stevens") or {}),
        )
        .call()
    )

    convert_to_user_timezone_stevens = (
        convert_values_to_timezone.validate()
        .set_task_instance_id("convert_to_user_timezone_stevens")
        .handle_errors()
        .with_tracing()
        .partial(
            df=process_columns_stevens,
            timezone=get_timezone,
            columns=["time"],
            **(params_dict.get("convert_to_user_timezone_stevens") or {}),
        )
        .call()
    )

    normalize_obs_details_stevens = (
        normalize_json_column.validate()
        .set_task_instance_id("normalize_obs_details_stevens")
        .handle_errors()
        .with_tracing()
        .partial(
            df=convert_to_user_timezone_stevens,
            column="observation_details",
            skip_if_not_exists=False,
            **(params_dict.get("normalize_obs_details_stevens") or {}),
        )
        .call()
    )

    drop_obs_details_prefix_stevens = (
        drop_column_prefix.validate()
        .set_task_instance_id("drop_obs_details_prefix_stevens")
        .handle_errors()
        .with_tracing()
        .partial(
            df=normalize_obs_details_stevens,
            prefix="observation_details__",
            duplicate_strategy="suffix",
            **(params_dict.get("drop_obs_details_prefix_stevens") or {}),
        )
        .call()
    )

    extract_date_stevens = (
        extract_column_as_type.validate()
        .set_task_instance_id("extract_date_stevens")
        .handle_errors()
        .with_tracing()
        .partial(
            df=drop_obs_details_prefix_stevens,
            column_name="recorded_at",
            output_type="date",
            output_column_name="date",
            **(params_dict.get("extract_date_stevens") or {}),
        )
        .call()
    )

    df_with_temporal_index = (
        add_temporal_index.validate()
        .set_task_instance_id("df_with_temporal_index")
        .handle_errors()
        .with_tracing()
        .partial(
            df=extract_date_stevens,
            time_col="recorded_at",
            groupers=groupers,
            cast_to_datetime=True,
            format="mixed",
            **(params_dict.get("df_with_temporal_index") or {}),
        )
        .call()
    )

    split_river_groups = (
        split_groups.validate()
        .set_task_instance_id("split_river_groups")
        .handle_errors()
        .with_tracing()
        .partial(
            df=df_with_temporal_index,
            groupers=groupers,
            **(params_dict.get("split_river_groups") or {}),
        )
        .call()
    )

    persist_stevens_observations = (
        persist_df_wrapper.validate()
        .set_task_instance_id("persist_stevens_observations")
        .handle_errors()
        .with_tracing()
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_stevens_observations") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_river_groups)
    )

    extract_river_values = (
        apply_sql_query.validate()
        .set_task_instance_id("extract_river_values")
        .handle_errors()
        .with_tracing()
        .partial(
            columns=["id", "DO", "Depth m", "date", "sensor"],
            query='SELECT CASE WHEN INSTR("DO", CHAR(32)) > 0 THEN CAST(SUBSTR("DO", 1, INSTR("DO", CHAR(32)) - 1) AS REAL) ELSE NULL END AS do_value, CASE WHEN INSTR("Depth m", CHAR(32)) > 0 THEN CAST(SUBSTR("Depth m", 1, INSTR("Depth m", CHAR(32)) - 1) AS REAL) ELSE NULL END AS depth_m_value, date, id FROM df WHERE sensor = "Mara River Purungat Bridge - Sensor M 20"',
            **(params_dict.get("extract_river_values") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=split_river_groups)
    )

    daily_river = (
        summarize_df.validate()
        .set_task_instance_id("daily_river")
        .handle_errors()
        .with_tracing()
        .partial(
            groupby_cols=["date"],
            summary_params=[
                {
                    "display_name": "Depth (m)",
                    "aggregator": "mean",
                    "column": "depth_m_value",
                },
                {
                    "display_name": "Dissolved Oxygen (mg/L)",
                    "aggregator": "mean",
                    "column": "do_value",
                },
            ],
            reset_index=True,
            **(params_dict.get("daily_river") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=extract_river_values)
    )

    persist_daily_summary_stevens = (
        persist_df_wrapper.validate()
        .set_task_instance_id("persist_daily_summary_stevens")
        .handle_errors()
        .with_tracing()
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            filetypes=["csv"],
            **(params_dict.get("persist_daily_summary_stevens") or {}),
        )
        .mapvalues(argnames=["df"], argvalues=daily_river)
    )

    depth_chart = (
        draw_line_chart.validate()
        .set_task_instance_id("depth_chart")
        .handle_errors()
        .with_tracing()
        .partial(
            x_column="date",
            y_column="Depth (m)",
            line_kwargs={"shape": "spline"},
            layout_kwargs={
                "xaxis": {"title": "Date"},
                "yaxis": {"title": "Depth (m)"},
                "legend_title": "Weather Station",
                "hovermode": "closest",
            },
            category_column="",
            **(params_dict.get("depth_chart") or {}),
        )
        .mapvalues(argnames=["dataframe"], argvalues=daily_river)
    )

    persist_depth = (
        persist_text.validate()
        .set_task_instance_id("persist_depth")
        .handle_errors()
        .with_tracing()
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_depth") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=depth_chart)
    )

    depth_chart_widget = (
        create_plot_widget_single_view.validate()
        .set_task_instance_id("depth_chart_widget")
        .handle_errors()
        .with_tracing()
        .partial(
            title="Daily River Flow", **(params_dict.get("depth_chart_widget") or {})
        )
        .map(argnames=["view", "data"], argvalues=persist_depth)
    )

    grouped_depth_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("grouped_depth_widget")
        .handle_errors()
        .with_tracing()
        .partial(
            widgets=depth_chart_widget,
            **(params_dict.get("grouped_depth_widget") or {}),
        )
        .call()
    )

    do_chart = (
        draw_line_chart.validate()
        .set_task_instance_id("do_chart")
        .handle_errors()
        .with_tracing()
        .partial(
            x_column="date",
            y_column="Dissolved Oxygen (mg/L)",
            line_kwargs={"shape": "spline"},
            layout_kwargs={
                "xaxis": {"title": "Date"},
                "yaxis": {"title": "Average Daily Dissolved Oxygen (mg/L)"},
                "legend_title": "Weather Station",
                "hovermode": "closest",
            },
            category_column="",
            **(params_dict.get("do_chart") or {}),
        )
        .mapvalues(argnames=["dataframe"], argvalues=daily_river)
    )

    persist_do = (
        persist_text.validate()
        .set_task_instance_id("persist_do")
        .handle_errors()
        .with_tracing()
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_do") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=do_chart)
    )

    do_chart_widget = (
        create_plot_widget_single_view.validate()
        .set_task_instance_id("do_chart_widget")
        .handle_errors()
        .with_tracing()
        .partial(
            title="Daily Dissolved Oxygen", **(params_dict.get("do_chart_widget") or {})
        )
        .map(argnames=["view", "data"], argvalues=persist_do)
    )

    grouped_do_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("grouped_do_widget")
        .handle_errors()
        .with_tracing()
        .partial(
            widgets=do_chart_widget, **(params_dict.get("grouped_do_widget") or {})
        )
        .call()
    )

    weather_dashboard = (
        gather_dashboard.validate()
        .set_task_instance_id("weather_dashboard")
        .handle_errors()
        .with_tracing()
        .partial(
            details=workflow_details,
            widgets=[grouped_depth_widget, grouped_do_widget],
            time_range=time_range,
            groupers=groupers,
            **(params_dict.get("weather_dashboard") or {}),
        )
        .call()
    )

    return weather_dashboard
