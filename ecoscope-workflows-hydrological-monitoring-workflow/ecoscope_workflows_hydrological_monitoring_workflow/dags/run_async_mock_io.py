# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details

# ruff: noqa: E402

"""WARNING: This file is generated in a testing context and should not be used in production.
Lines specific to the testing context are marked with a test tube emoji (ðŸ§ª) to indicate
that they would not be included (or would be different) in the production version of this file.
"""

import json
import os
import warnings  # ðŸ§ª

from ecoscope_workflows_core.graph import DependsOn, Graph, Node
from ecoscope_workflows_core.tasks.config import (
    set_workflow_details as set_workflow_details,
)
from ecoscope_workflows_core.tasks.filter import (
    get_timezone_from_time_range as get_timezone_from_time_range,
)
from ecoscope_workflows_core.tasks.filter import set_time_range as set_time_range
from ecoscope_workflows_core.tasks.io import set_er_connection as set_er_connection
from ecoscope_workflows_core.testing import create_task_magicmock  # ðŸ§ª

get_subjectgroup_observations = create_task_magicmock(  # ðŸ§ª
    anchor="ecoscope_workflows_ext_ecoscope.tasks.io",  # ðŸ§ª
    func_name="get_subjectgroup_observations",  # ðŸ§ª
)  # ðŸ§ª
from ecoscope_workflows_core.tasks.groupby import set_groupers as set_groupers
from ecoscope_workflows_core.tasks.groupby import split_groups as split_groups
from ecoscope_workflows_core.tasks.io import persist_text as persist_text
from ecoscope_workflows_core.tasks.results import (
    create_plot_widget_single_view as create_plot_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import (
    create_table_widget_single_view as create_table_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import gather_dashboard as gather_dashboard
from ecoscope_workflows_core.tasks.results import (
    merge_widget_views as merge_widget_views,
)
from ecoscope_workflows_core.tasks.transformation import (
    add_temporal_index as add_temporal_index,
)
from ecoscope_workflows_core.tasks.transformation import (
    convert_values_to_timezone as convert_values_to_timezone,
)
from ecoscope_workflows_core.tasks.transformation import (
    extract_column_as_type as extract_column_as_type,
)
from ecoscope_workflows_core.tasks.transformation import map_columns as map_columns
from ecoscope_workflows_ext_custom.tasks.io import (
    persist_df_wrapper as persist_df_wrapper,
)
from ecoscope_workflows_ext_custom.tasks.results import create_docx as create_docx
from ecoscope_workflows_ext_custom.tasks.transformation import (
    apply_sql_query as apply_sql_query,
)
from ecoscope_workflows_ext_custom.tasks.transformation import (
    drop_column_prefix as drop_column_prefix,
)
from ecoscope_workflows_ext_ecoscope.tasks.analysis import summarize_df as summarize_df
from ecoscope_workflows_ext_ecoscope.tasks.results import (
    draw_line_chart as draw_line_chart,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import draw_table as draw_table
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    normalize_json_column as normalize_json_column,
)

from ..params import Params


def main(params: Params):
    warnings.warn("This test script should not be used in production!")  # ðŸ§ª

    params_dict = json.loads(params.model_dump_json(exclude_unset=True))

    dependencies = {
        "workflow_details": [],
        "time_range": [],
        "get_timezone": ["time_range"],
        "er_client_name": [],
        "subject_obs_stevens": ["er_client_name", "time_range"],
        "groupers": [],
        "drop_extra_prefix_stevens": ["subject_obs_stevens"],
        "process_columns_stevens": ["drop_extra_prefix_stevens"],
        "convert_to_user_timezone_stevens": ["process_columns_stevens", "get_timezone"],
        "normalize_obs_details_stevens": ["convert_to_user_timezone_stevens"],
        "drop_obs_details_prefix_stevens": ["normalize_obs_details_stevens"],
        "extract_date_stevens": ["drop_obs_details_prefix_stevens"],
        "df_with_temporal_index": ["extract_date_stevens", "groupers"],
        "split_river_groups": ["df_with_temporal_index", "groupers"],
        "persist_stevens_observations": ["split_river_groups"],
        "extract_river_values": ["split_river_groups"],
        "daily_river": ["extract_river_values"],
        "persist_daily_summary_stevens": ["daily_river"],
        "draw_summary_table": ["daily_river"],
        "persist_summary_table": ["draw_summary_table"],
        "summary_table_widget": ["persist_summary_table"],
        "grouped_summary_table_widget": ["summary_table_widget"],
        "depth_chart": ["daily_river"],
        "persist_depth": ["depth_chart"],
        "depth_chart_widget": ["persist_depth"],
        "grouped_depth_widget": ["depth_chart_widget"],
        "do_chart": ["daily_river"],
        "persist_do": ["do_chart"],
        "do_chart_widget": ["persist_do"],
        "grouped_do_widget": ["do_chart_widget"],
        "create_hydrological_report": [
            "time_range",
            "persist_depth",
            "persist_do",
            "groupers",
        ],
        "weather_dashboard": [
            "workflow_details",
            "grouped_depth_widget",
            "grouped_do_widget",
            "grouped_summary_table_widget",
            "time_range",
            "groupers",
        ],
    }

    nodes = {
        "workflow_details": Node(
            async_task=set_workflow_details.validate()
            .set_task_instance_id("workflow_details")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial=(params_dict.get("workflow_details") or {}),
            method="call",
        ),
        "time_range": Node(
            async_task=set_time_range.validate()
            .set_task_instance_id("time_range")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "time_format": "%d %b %Y %H:%M:%S %Z",
            }
            | (params_dict.get("time_range") or {}),
            method="call",
        ),
        "get_timezone": Node(
            async_task=get_timezone_from_time_range.validate()
            .set_task_instance_id("get_timezone")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "time_range": DependsOn("time_range"),
            }
            | (params_dict.get("get_timezone") or {}),
            method="call",
        ),
        "er_client_name": Node(
            async_task=set_er_connection.validate()
            .set_task_instance_id("er_client_name")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial=(params_dict.get("er_client_name") or {}),
            method="call",
        ),
        "subject_obs_stevens": Node(
            async_task=get_subjectgroup_observations.validate()
            .set_task_instance_id("subject_obs_stevens")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "client": DependsOn("er_client_name"),
                "time_range": DependsOn("time_range"),
                "raise_on_empty": True,
                "include_details": True,
                "include_subjectsource_details": True,
                "filter": "none",
            }
            | (params_dict.get("subject_obs_stevens") or {}),
            method="call",
        ),
        "groupers": Node(
            async_task=set_groupers.validate()
            .set_task_instance_id("groupers")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial=(params_dict.get("groupers") or {}),
            method="call",
        ),
        "drop_extra_prefix_stevens": Node(
            async_task=drop_column_prefix.validate()
            .set_task_instance_id("drop_extra_prefix_stevens")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "df": DependsOn("subject_obs_stevens"),
                "prefix": "extra__",
                "duplicate_strategy": "suffix",
            }
            | (params_dict.get("drop_extra_prefix_stevens") or {}),
            method="call",
        ),
        "process_columns_stevens": Node(
            async_task=map_columns.validate()
            .set_task_instance_id("process_columns_stevens")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "df": DependsOn("drop_extra_prefix_stevens"),
                "retain_columns": [
                    "id",
                    "subject_id",
                    "created_at",
                    "recorded_at",
                    "source",
                    "device_status_properties",
                    "observation_details",
                    "geometry",
                    "subject__name",
                ],
                "rename_columns": {
                    "subject__name": "sensor",
                },
                "raise_if_not_found": False,
            }
            | (params_dict.get("process_columns_stevens") or {}),
            method="call",
        ),
        "convert_to_user_timezone_stevens": Node(
            async_task=convert_values_to_timezone.validate()
            .set_task_instance_id("convert_to_user_timezone_stevens")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "df": DependsOn("process_columns_stevens"),
                "timezone": DependsOn("get_timezone"),
                "columns": [
                    "time",
                ],
            }
            | (params_dict.get("convert_to_user_timezone_stevens") or {}),
            method="call",
        ),
        "normalize_obs_details_stevens": Node(
            async_task=normalize_json_column.validate()
            .set_task_instance_id("normalize_obs_details_stevens")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "df": DependsOn("convert_to_user_timezone_stevens"),
                "column": "observation_details",
                "skip_if_not_exists": False,
                "sort_columns": True,
            }
            | (params_dict.get("normalize_obs_details_stevens") or {}),
            method="call",
        ),
        "drop_obs_details_prefix_stevens": Node(
            async_task=drop_column_prefix.validate()
            .set_task_instance_id("drop_obs_details_prefix_stevens")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "df": DependsOn("normalize_obs_details_stevens"),
                "prefix": "observation_details__",
                "duplicate_strategy": "suffix",
            }
            | (params_dict.get("drop_obs_details_prefix_stevens") or {}),
            method="call",
        ),
        "extract_date_stevens": Node(
            async_task=extract_column_as_type.validate()
            .set_task_instance_id("extract_date_stevens")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "df": DependsOn("drop_obs_details_prefix_stevens"),
                "column_name": "recorded_at",
                "output_type": "date",
                "output_column_name": "date",
            }
            | (params_dict.get("extract_date_stevens") or {}),
            method="call",
        ),
        "df_with_temporal_index": Node(
            async_task=add_temporal_index.validate()
            .set_task_instance_id("df_with_temporal_index")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "df": DependsOn("extract_date_stevens"),
                "time_col": "recorded_at",
                "groupers": DependsOn("groupers"),
                "cast_to_datetime": True,
                "format": "mixed",
            }
            | (params_dict.get("df_with_temporal_index") or {}),
            method="call",
        ),
        "split_river_groups": Node(
            async_task=split_groups.validate()
            .set_task_instance_id("split_river_groups")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "df": DependsOn("df_with_temporal_index"),
                "groupers": DependsOn("groupers"),
            }
            | (params_dict.get("split_river_groups") or {}),
            method="call",
        ),
        "persist_stevens_observations": Node(
            async_task=persist_df_wrapper.validate()
            .set_task_instance_id("persist_stevens_observations")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "root_path": os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
                "sanitize": True,
            }
            | (params_dict.get("persist_stevens_observations") or {}),
            method="mapvalues",
            kwargs={
                "argnames": ["df"],
                "argvalues": DependsOn("split_river_groups"),
            },
        ),
        "extract_river_values": Node(
            async_task=apply_sql_query.validate()
            .set_task_instance_id("extract_river_values")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "columns": [
                    "id",
                    "DO",
                    "Depth m",
                    "date",
                    "sensor",
                ],
                "query": 'SELECT CASE WHEN INSTR("DO", CHAR(32)) > 0 THEN CAST(SUBSTR("DO", 1, INSTR("DO", CHAR(32)) - 1) AS REAL) ELSE NULL END AS do_value, CASE WHEN INSTR("Depth m", CHAR(32)) > 0 THEN CAST(SUBSTR("Depth m", 1, INSTR("Depth m", CHAR(32)) - 1) AS REAL) ELSE NULL END AS depth_m_value, date, id FROM df WHERE sensor = "Mara River Purungat Bridge - Sensor M 20"',
            }
            | (params_dict.get("extract_river_values") or {}),
            method="mapvalues",
            kwargs={
                "argnames": ["df"],
                "argvalues": DependsOn("split_river_groups"),
            },
        ),
        "daily_river": Node(
            async_task=summarize_df.validate()
            .set_task_instance_id("daily_river")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "groupby_cols": [
                    "date",
                ],
                "summary_params": [
                    {
                        "display_name": "Depth (m)",
                        "aggregator": "mean",
                        "column": "depth_m_value",
                    },
                    {
                        "display_name": "Dissolved Oxygen (mg/L)",
                        "aggregator": "mean",
                        "column": "do_value",
                    },
                ],
                "reset_index": True,
            }
            | (params_dict.get("daily_river") or {}),
            method="mapvalues",
            kwargs={
                "argnames": ["df"],
                "argvalues": DependsOn("extract_river_values"),
            },
        ),
        "persist_daily_summary_stevens": Node(
            async_task=persist_df_wrapper.validate()
            .set_task_instance_id("persist_daily_summary_stevens")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "root_path": os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
                "filetypes": [
                    "csv",
                ],
                "sanitize": True,
            }
            | (params_dict.get("persist_daily_summary_stevens") or {}),
            method="mapvalues",
            kwargs={
                "argnames": ["df"],
                "argvalues": DependsOn("daily_river"),
            },
        ),
        "draw_summary_table": Node(
            async_task=draw_table.validate()
            .set_task_instance_id("draw_summary_table")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "table_config": {
                    "enable_sorting": True,
                    "enable_filtering": False,
                    "enable_download": False,
                    "hide_header": False,
                },
            }
            | (params_dict.get("draw_summary_table") or {}),
            method="mapvalues",
            kwargs={
                "argnames": ["dataframe"],
                "argvalues": DependsOn("daily_river"),
            },
        ),
        "persist_summary_table": Node(
            async_task=persist_text.validate()
            .set_task_instance_id("persist_summary_table")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "root_path": os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            }
            | (params_dict.get("persist_summary_table") or {}),
            method="mapvalues",
            kwargs={
                "argnames": ["text"],
                "argvalues": DependsOn("draw_summary_table"),
            },
        ),
        "summary_table_widget": Node(
            async_task=create_table_widget_single_view.validate()
            .set_task_instance_id("summary_table_widget")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "title": "Daily Summary",
            }
            | (params_dict.get("summary_table_widget") or {}),
            method="map",
            kwargs={
                "argnames": ["view", "data"],
                "argvalues": DependsOn("persist_summary_table"),
            },
        ),
        "grouped_summary_table_widget": Node(
            async_task=merge_widget_views.validate()
            .set_task_instance_id("grouped_summary_table_widget")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "widgets": DependsOn("summary_table_widget"),
            }
            | (params_dict.get("grouped_summary_table_widget") or {}),
            method="call",
        ),
        "depth_chart": Node(
            async_task=draw_line_chart.validate()
            .set_task_instance_id("depth_chart")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "x_column": "date",
                "y_column": "Depth (m)",
                "line_kwargs": {
                    "shape": "spline",
                },
                "layout_kwargs": {
                    "xaxis": {
                        "title": "Date",
                    },
                    "yaxis": {
                        "title": "Depth (m)",
                    },
                    "legend_title": "Weather Station",
                    "hovermode": "closest",
                },
                "category_column": "",
                "smoothing": None,
            }
            | (params_dict.get("depth_chart") or {}),
            method="mapvalues",
            kwargs={
                "argnames": ["dataframe"],
                "argvalues": DependsOn("daily_river"),
            },
        ),
        "persist_depth": Node(
            async_task=persist_text.validate()
            .set_task_instance_id("persist_depth")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "root_path": os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            }
            | (params_dict.get("persist_depth") or {}),
            method="mapvalues",
            kwargs={
                "argnames": ["text"],
                "argvalues": DependsOn("depth_chart"),
            },
        ),
        "depth_chart_widget": Node(
            async_task=create_plot_widget_single_view.validate()
            .set_task_instance_id("depth_chart_widget")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "title": "Daily River Flow",
            }
            | (params_dict.get("depth_chart_widget") or {}),
            method="map",
            kwargs={
                "argnames": ["view", "data"],
                "argvalues": DependsOn("persist_depth"),
            },
        ),
        "grouped_depth_widget": Node(
            async_task=merge_widget_views.validate()
            .set_task_instance_id("grouped_depth_widget")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "widgets": DependsOn("depth_chart_widget"),
            }
            | (params_dict.get("grouped_depth_widget") or {}),
            method="call",
        ),
        "do_chart": Node(
            async_task=draw_line_chart.validate()
            .set_task_instance_id("do_chart")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "x_column": "date",
                "y_column": "Dissolved Oxygen (mg/L)",
                "line_kwargs": {
                    "shape": "spline",
                },
                "layout_kwargs": {
                    "xaxis": {
                        "title": "Date",
                    },
                    "yaxis": {
                        "title": "Average Daily Dissolved Oxygen (mg/L)",
                    },
                    "legend_title": "Weather Station",
                    "hovermode": "closest",
                },
                "category_column": "",
                "smoothing": None,
            }
            | (params_dict.get("do_chart") or {}),
            method="mapvalues",
            kwargs={
                "argnames": ["dataframe"],
                "argvalues": DependsOn("daily_river"),
            },
        ),
        "persist_do": Node(
            async_task=persist_text.validate()
            .set_task_instance_id("persist_do")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "root_path": os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            }
            | (params_dict.get("persist_do") or {}),
            method="mapvalues",
            kwargs={
                "argnames": ["text"],
                "argvalues": DependsOn("do_chart"),
            },
        ),
        "do_chart_widget": Node(
            async_task=create_plot_widget_single_view.validate()
            .set_task_instance_id("do_chart_widget")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "title": "Daily Dissolved Oxygen",
            }
            | (params_dict.get("do_chart_widget") or {}),
            method="map",
            kwargs={
                "argnames": ["view", "data"],
                "argvalues": DependsOn("persist_do"),
            },
        ),
        "grouped_do_widget": Node(
            async_task=merge_widget_views.validate()
            .set_task_instance_id("grouped_do_widget")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "widgets": DependsOn("do_chart_widget"),
            }
            | (params_dict.get("grouped_do_widget") or {}),
            method="call",
        ),
        "create_hydrological_report": Node(
            async_task=create_docx.validate()
            .set_task_instance_id("create_hydrological_report")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "context": {
                    "items": [
                        {
                            "item_type": "timerange",
                            "key": "report_date",
                            "value": DependsOn("time_range"),
                        },
                        {
                            "item_type": "image",
                            "key": "depth_chart",
                            "value": DependsOn("persist_depth"),
                            "screenshot_config": {
                                "wait_for_timeout": 0,
                            },
                        },
                        {
                            "item_type": "image",
                            "key": "do_chart",
                            "value": DependsOn("persist_do"),
                            "screenshot_config": {
                                "wait_for_timeout": 0,
                            },
                        },
                    ],
                },
                "groupers": DependsOn("groupers"),
                "output_dir": os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
                "filename_prefix": "hydrological_report",
            }
            | (params_dict.get("create_hydrological_report") or {}),
            method="call",
        ),
        "weather_dashboard": Node(
            async_task=gather_dashboard.validate()
            .set_task_instance_id("weather_dashboard")
            .handle_errors()
            .with_tracing()
            .set_executor("lithops"),
            partial={
                "details": DependsOn("workflow_details"),
                "widgets": [
                    DependsOn("grouped_depth_widget"),
                    DependsOn("grouped_do_widget"),
                    DependsOn("grouped_summary_table_widget"),
                ],
                "time_range": DependsOn("time_range"),
                "groupers": DependsOn("groupers"),
            }
            | (params_dict.get("weather_dashboard") or {}),
            method="call",
        ),
    }
    graph = Graph(dependencies=dependencies, nodes=nodes)
    results = graph.execute()
    return results
